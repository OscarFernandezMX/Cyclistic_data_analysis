{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3bd1a6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e7a7a0",
   "metadata": {},
   "source": [
    "We create a Spark session. To monitor, visit <http://localhost:4040/jobs/>.  \n",
    "Then, we load the data and create a temporary view to work with SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48b3b7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/11 21:37:02 WARN Utils: Your hostname, DS-A90101.local resolves to a loopback address: 127.0.0.1; using 192.168.100.217 instead (on interface en0)\n",
      "24/02/11 21:37:02 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/11 21:37:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "24/02/11 21:37:20 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Spark session.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Cyclistic cleaning data\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load data.\n",
    "df = spark.read.csv(\"Data/cyclistic/2023_cyclistic_tripdata.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Create temporary view for SQL queries.\n",
    "df.createOrReplaceTempView(\"cyclistic_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1321faad",
   "metadata": {},
   "source": [
    "## Checking columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d9443",
   "metadata": {},
   "source": [
    "### `ride_id`\n",
    "Count the `ride_id` and compare with the total rows. We can see they match, we have no repeated ids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b03b8282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/11 15:07:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:18 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 15:07:23 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rows</th>\n",
       "      <th>Unique_IDs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5719877</td>\n",
       "      <td>5719877</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rows  Unique_IDs\n",
       "0  5719877     5719877"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(*) AS Rows,\n",
    "    COUNT(DISTINCT ride_id) AS Unique_IDs\n",
    "FROM\n",
    "    cyclistic_data;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a0bf77",
   "metadata": {},
   "source": [
    "### `rideable_type`\n",
    "\n",
    "We can see that there are three unique bicycle types. This columns is also clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04e6aebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rideable_type</th>\n",
       "      <th>Rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>electric_bike</td>\n",
       "      <td>2945579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>classic_bike</td>\n",
       "      <td>2696011</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>docked_bike</td>\n",
       "      <td>78287</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   rideable_type    Rides\n",
       "0  electric_bike  2945579\n",
       "1   classic_bike  2696011\n",
       "2    docked_bike    78287"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    rideable_type,\n",
    "    COUNT(*) AS Rides\n",
    "FROM\n",
    "    cyclistic_data\n",
    "GROUP BY\n",
    "    rideable_type\n",
    "ORDER BY\n",
    "    Rides DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3db419",
   "metadata": {},
   "source": [
    "### Dates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6f7e7b",
   "metadata": {},
   "source": [
    "We can see that in both columns, there are entries from the year 2022, and rides that ended in 2024. Since we are interested in data from 2023, we'll remove these entries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c499620",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>min(started_at)</th>\n",
       "      <th>max(started_at)</th>\n",
       "      <th>min(ended_at)</th>\n",
       "      <th>max(ended_at)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-12-31 18:01:58</td>\n",
       "      <td>2023-12-31 17:59:38</td>\n",
       "      <td>2022-12-31 18:02:41</td>\n",
       "      <td>2024-01-01 17:50:51</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      min(started_at)     max(started_at)       min(ended_at)  \\\n",
       "0 2022-12-31 18:01:58 2023-12-31 17:59:38 2022-12-31 18:02:41   \n",
       "\n",
       "        max(ended_at)  \n",
       "0 2024-01-01 17:50:51  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    MIN(started_at),\n",
    "    MAX(started_at),\n",
    "    MIN(ended_at),\n",
    "    MAX(ended_at)\n",
    "FROM\n",
    "    cyclistic_data;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f793f1",
   "metadata": {},
   "source": [
    "#### Removing entries that are not from the year 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44158f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    cyclistic_data\n",
    "WHERE\n",
    "    (started_at >= \"2023-01-01 00:00:00\" AND\n",
    "    started_at <= \"2023-12-31 23:59:59\") AND\n",
    "    (ended_at >= \"2023-01-01 00:00:00\" AND\n",
    "    ended_at <= \"2023-12-31 23:59:59\")\n",
    "ORDER BY\n",
    "    started_at ASC;\n",
    "\"\"\"\n",
    "\n",
    "df2 = spark.sql(query)\n",
    "df2.createOrReplaceTempView(\"cyclistic_data_2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1ade3be",
   "metadata": {},
   "source": [
    "After removing those entries, our table has 5,718,838 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a76f9e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5718838</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rows\n",
       "0  5718838"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(*) AS Rows\n",
    "FROM\n",
    "    cyclistic_data_2\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b7b9a6d",
   "metadata": {},
   "source": [
    "### Station ids and names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38ef827",
   "metadata": {},
   "source": [
    "We found two issues:\n",
    "- There is no information about some stations.\n",
    "- The naming of the stations looks inconsistent in some cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d798eec",
   "metadata": {},
   "source": [
    "#### Removing NAs\n",
    "\n",
    "We can see that there are rides where we don't have the start information (id, name, latitude, and longitude).  \n",
    "For simplicity in the analysis, we assume that the data is not enough to infer more information about these stations with the available data (i.e. we cannot infer with latitud and longitude coordinates or station id the name of a missing station)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48de954b",
   "metadata": {},
   "source": [
    "This is the data that we want to remove."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b97db454",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    cyclistic_data_2\n",
    "WHERE\n",
    "    start_station_name = \"NA\" OR\n",
    "    start_station_id = \"NA\" OR\n",
    "    start_lat = \"NA\" OR\n",
    "    start_lng = \"NA\" OR\n",
    "    end_station_name = \"NA\" OR\n",
    "    end_station_id = \"NA\" OR\n",
    "    end_lat = \"NA\" OR\n",
    "    end_lng = \"NA\"\n",
    "\"\"\"\n",
    "\n",
    "aux_df = spark.sql(query)\n",
    "aux_df.createOrReplaceTempView(\"aux\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d01883",
   "metadata": {},
   "source": [
    "Now we remove it from the table and save it into a new one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e26c7297",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    *\n",
    "FROM\n",
    "    cyclistic_data_2\n",
    "WHERE NOT EXISTS (\n",
    "    SELECT 1\n",
    "    FROM aux\n",
    "    WHERE cyclistic_data_2.ride_id = aux.ride_id\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "df3 = spark.sql(query)\n",
    "df3.createOrReplaceTempView(\"cyclistic_data_3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47fbc29",
   "metadata": {},
   "source": [
    "After removing NA rows, 4,330,969 is the new size of our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "827c22fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4330969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rows\n",
       "0  4330969"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(*) AS Rows\n",
    "FROM\n",
    "    cyclistic_data_3\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57007ed7",
   "metadata": {},
   "source": [
    "#### Character length\n",
    "\n",
    "It seems that the name is the same as the id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf13834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:51:59 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:52:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "24/02/11 21:52:00 WARN RowBasedKeyValueBatch: Calling spill() on RowBasedKeyValueBatch. Will not spill but return 0.\n",
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>same_name_and_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>410</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       start_station_name  same_name_and_id\n",
       "0  OH Charging Stx - Test                13\n",
       "1                     410                 3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    start_station_name,\n",
    "    SUM(CASE\n",
    "        WHEN start_station_name = start_station_id THEN 1\n",
    "        ELSE 0\n",
    "    END) AS same_name_and_id\n",
    "FROM\n",
    "    cyclistic_data_3\n",
    "GROUP BY\n",
    "    start_station_name\n",
    "HAVING\n",
    "    same_name_and_id > 0\n",
    "LIMIT\n",
    "    10;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2f7e04d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>same_name_and_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Green St &amp; Washington Blvd</td>\n",
       "      <td>13053</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Rush St &amp; Superior St</td>\n",
       "      <td>15530</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Halsted St &amp; Roscoe St</td>\n",
       "      <td>TA1309000025</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>State St &amp; 33rd St</td>\n",
       "      <td>13216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wabash Ave &amp; Roosevelt Rd</td>\n",
       "      <td>TA1305000002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>State St &amp; 33rd St</td>\n",
       "      <td>13216</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Morgan Ave &amp; 14th Pl</td>\n",
       "      <td>TA1306000002</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Elston Ave &amp; Wabansia Ave</td>\n",
       "      <td>TA1309000032</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Wabash Ave &amp; 16th St</td>\n",
       "      <td>SL-012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Southport Ave &amp; Wellington Ave</td>\n",
       "      <td>TA1307000006</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Western Ave &amp; 21st St</td>\n",
       "      <td>13091</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Desplaines St &amp; Kinzie St</td>\n",
       "      <td>TA1306000003</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Clark St &amp; North Ave</td>\n",
       "      <td>13128</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Halsted St &amp; 111th St</td>\n",
       "      <td>20127</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Michigan Ave &amp; Ida B Wells Dr</td>\n",
       "      <td>TA1305000010</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Public Rack - Woodlawn Ave &amp; 63rd St N</td>\n",
       "      <td>951</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Jefferson St &amp; Monroe St</td>\n",
       "      <td>WL-011</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Lakeview Ave &amp; Fullerton Pkwy</td>\n",
       "      <td>TA1309000019</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Clark St &amp; Elm St</td>\n",
       "      <td>TA1307000039</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Orleans St &amp; Chestnut St (NEXT Apts)</td>\n",
       "      <td>620</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        start_station_name start_station_id  same_name_and_id\n",
       "0               Green St & Washington Blvd            13053                 0\n",
       "1                    Rush St & Superior St            15530                 0\n",
       "2                   Halsted St & Roscoe St     TA1309000025                 0\n",
       "3                       State St & 33rd St            13216                 0\n",
       "4                Wabash Ave & Roosevelt Rd     TA1305000002                 0\n",
       "5                       State St & 33rd St            13216                 0\n",
       "6                     Morgan Ave & 14th Pl     TA1306000002                 0\n",
       "7                Elston Ave & Wabansia Ave     TA1309000032                 0\n",
       "8                     Wabash Ave & 16th St           SL-012                 0\n",
       "9           Southport Ave & Wellington Ave     TA1307000006                 0\n",
       "10                   Western Ave & 21st St            13091                 0\n",
       "11               Desplaines St & Kinzie St     TA1306000003                 0\n",
       "12                    Clark St & North Ave            13128                 0\n",
       "13                   Halsted St & 111th St            20127                 0\n",
       "14           Michigan Ave & Ida B Wells Dr     TA1305000010                 0\n",
       "15  Public Rack - Woodlawn Ave & 63rd St N              951                 0\n",
       "16                Jefferson St & Monroe St           WL-011                 0\n",
       "17           Lakeview Ave & Fullerton Pkwy     TA1309000019                 0\n",
       "18                       Clark St & Elm St     TA1307000039                 0\n",
       "19    Orleans St & Chestnut St (NEXT Apts)              620                 0"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    start_station_name,\n",
    "    start_station_id,\n",
    "    CASE\n",
    "        WHEN start_station_name = start_station_id THEN 1\n",
    "        ELSE 0\n",
    "    END AS same_name_and_id\n",
    "FROM\n",
    "    cyclistic_data_3\n",
    "WHERE\n",
    "    CASE\n",
    "        WHEN start_station_name = start_station_id THEN 1\n",
    "        ELSE 0\n",
    "    END = 0\n",
    "LIMIT\n",
    "    20;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8118226a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>start_station_name</th>\n",
       "      <th>start_station_id</th>\n",
       "      <th>same_name_and_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>OH Charging Stx - Test</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        start_station_name        start_station_id  same_name_and_id\n",
       "0   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "1   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "2   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "3   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "4   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "5   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "6   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "7   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "8   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "9   OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "10  OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "11  OH Charging Stx - Test  OH Charging Stx - Test                 1\n",
       "12  OH Charging Stx - Test  OH Charging Stx - Test                 1"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    start_station_name,\n",
    "    start_station_id,\n",
    "    CASE\n",
    "        WHEN start_station_name = start_station_id THEN 1\n",
    "        ELSE 0\n",
    "    END AS same_name_and_id\n",
    "FROM\n",
    "    cyclistic_data_3\n",
    "WHERE\n",
    "    start_station_name = \"OH Charging Stx - Test\"\n",
    "LIMIT\n",
    "    20;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4307386",
   "metadata": {},
   "source": [
    "### `member_casual`\n",
    "\n",
    "We can see that our labels are correct for this column. We expect only two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fe6c1a2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>member_casual</th>\n",
       "      <th>Rides</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>casual</td>\n",
       "      <td>2059179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>member</td>\n",
       "      <td>3660698</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  member_casual    Rides\n",
       "0        casual  2059179\n",
       "1        member  3660698"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    member_casual,\n",
    "    COUNT(ride_id) AS Rides\n",
    "FROM\n",
    "    cyclistic_data\n",
    "GROUP BY\n",
    "    member_casual;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8793a465",
   "metadata": {},
   "source": [
    "Now the data is clean and we can export it.  \n",
    "It is important to note that the unbalanced class problem is still present, but we'll solve it using R."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0429627e",
   "metadata": {},
   "source": [
    "To export the data, we need to repartition the data to get a single csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7271ca7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df3.repartition(1).write.csv(\"Data/cyclistic/2023_cyclistic_tripdata_clean\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4bab628e",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv Data/cyclistic/2023_cyclistic_tripdata_clean/*.csv Data/cyclistic/2023_cyclistic_tripdata_clean.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5c2824bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -r Data/cyclistic/2023_cyclistic_tripdata_clean/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "089bfc86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "202301-divvy-tripdata.csv         202308-divvy-tripdata.csv\r\n",
      "202302-divvy-tripdata.csv         202309-divvy-tripdata.csv\r\n",
      "202303-divvy-tripdata.csv         202310-divvy-tripdata.csv\r\n",
      "202304-divvy-tripdata.csv         202311-divvy-tripdata.csv\r\n",
      "202305-divvy-tripdata.csv         202312-divvy-tripdata.csv\r\n",
      "202306-divvy-tripdata.csv         2023_cyclistic_tripdata.csv\r\n",
      "202307-divvy-tripdata.csv         2023_cyclistic_tripdata_clean.csv\r\n"
     ]
    }
   ],
   "source": [
    "!ls Data/cyclistic/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d1b56f1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/11 15:19:39 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv(\"Data/cyclistic/2023_cyclistic_tripdata_clean.csv\", header=True, inferSchema=True)\n",
    "df.createOrReplaceTempView(\"cyclistic_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6fefa51d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Rows</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4330969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Rows\n",
       "0  4330969"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"\"\"\n",
    "SELECT\n",
    "    COUNT(*) AS Rows\n",
    "FROM\n",
    "    cyclistic_data;\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e2b79d91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ConnectionRefusedError",
     "evalue": "[Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/sql/session.py:1796\u001b[0m, in \u001b[0;36mSparkSession.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1782\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1783\u001b[0m \u001b[38;5;124;03mStop the underlying :class:`SparkContext`.\u001b[39;00m\n\u001b[1;32m   1784\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1792\u001b[0m \u001b[38;5;124;03m>>> spark.stop()  # doctest: +SKIP\u001b[39;00m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SQLContext\n\u001b[0;32m-> 1796\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1797\u001b[0m \u001b[38;5;66;03m# We should clean the default session up. See SPARK-23228.\u001b[39;00m\n\u001b[1;32m   1798\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pyspark/context.py:654\u001b[0m, in \u001b[0;36mSparkContext.stop\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_jsc\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    653\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 654\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    655\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JError:\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;66;03m# Case: SPARK-18523\u001b[39;00m\n\u001b[1;32m    657\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    658\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to cleanly shutdown Spark JVM process.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    659\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m It is possible that the process has crashed,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    660\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m been killed or may also be in a zombie state.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    661\u001b[0m             \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m,\n\u001b[1;32m    662\u001b[0m         )\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1314\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/java_gateway.py:1036\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_command\u001b[39m(\u001b[38;5;28mself\u001b[39m, command, retry\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m   1016\u001b[0m     \u001b[38;5;124;03m\"\"\"Sends a command to the JVM. This method is not intended to be\u001b[39;00m\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;124;03m       called directly by Py4J users. It is usually called by\u001b[39;00m\n\u001b[1;32m   1018\u001b[0m \u001b[38;5;124;03m       :class:`JavaMember` instances.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1034\u001b[0m \u001b[38;5;124;03m     if `binary` is `True`.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1036\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1037\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1038\u001b[0m         response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39msend_command(command)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/clientserver.py:284\u001b[0m, in \u001b[0;36mJavaClient._get_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m connection \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m connection\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_new_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/clientserver.py:291\u001b[0m, in \u001b[0;36mJavaClient._create_new_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_create_new_connection\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    288\u001b[0m     connection \u001b[38;5;241m=\u001b[39m ClientServerConnection(\n\u001b[1;32m    289\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_parameters, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_parameters,\n\u001b[1;32m    290\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_property, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 291\u001b[0m     \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_to_java_server\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_thread_connection(connection)\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m connection\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/py4j/clientserver.py:438\u001b[0m, in \u001b[0;36mClientServerConnection.connect_to_java_server\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context:\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mssl_context\u001b[38;5;241m.\u001b[39mwrap_socket(\n\u001b[1;32m    437\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket, server_hostname\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjava_address)\n\u001b[0;32m--> 438\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_address\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjava_port\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msocket\u001b[38;5;241m.\u001b[39mmakefile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    440\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_connected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mConnectionRefusedError\u001b[0m: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddaebfc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
